{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Importar bibliotecas necessárias\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.snowball import SnowballStemmer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOQ5n_kGxJUH",
        "outputId": "cbf53abf-0a83-4486-c5a1-dfb498a7fcb4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir função de pré-processamento\n",
        "def preprocess_dataset(df, language):\n",
        "\n",
        "    # Configurar stemmer e stopwords para o idioma\n",
        "    stemmer = SnowballStemmer(language)\n",
        "    stop_words = set(stopwords.words(language))\n",
        "\n",
        "    def preprocess_text(text):\n",
        "        # Converter para letras minúsculas\n",
        "        text = text.lower()\n",
        "        # Remover caracteres especiais, links e números\n",
        "        text = re.sub(r'http\\S+|www\\S+|[^a-zA-Z\\s]', '', text)\n",
        "        # Tokenizar o texto\n",
        "        tokens = word_tokenize(text)\n",
        "        # Remover stopwords\n",
        "        tokens = [word for word in tokens if word not in stop_words]\n",
        "        # Aplicar stemmização\n",
        "        tokens = [stemmer.stem(word) for word in tokens]\n",
        "        # Reconstruir o texto processado\n",
        "        return ' '.join(tokens)\n",
        "\n",
        "    # Aplicar a função de pré-processamento no dataset\n",
        "    df['processed_text'] = df['message'].apply(preprocess_text)\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "hKLl5ljAxN5h"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# Upload de arquivo local\n",
        "# Faça upload dos arquivos\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "Gg8qTuXOx6tZ",
        "outputId": "0996a49b-2906-4ad4-bfb9-5026fe12a3a4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Upload de arquivo local\\n# Faça upload dos arquivos\\n\\nfrom google.colab import files\\nuploaded = files.upload()\\nfor fn in uploaded.keys():\\n  print(\\'User uploaded file \"{name}\" with length {length} bytes\\'.format(\\n      name=fn, length=len(uploaded[fn])))\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregar os três datasets\n",
        "portuguese_df = pd.read_csv('dataset_pt.csv')\n",
        "english_df = pd.read_csv('dataset_en.csv')\n",
        "spanish_df = pd.read_csv('dataset_es.csv')\n",
        "\n",
        "# Exibir as primeiras linhas de cada dataset\n",
        "print(portuguese_df.head())\n",
        "print(english_df.head())\n",
        "print(spanish_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUkTRaKQxSiC",
        "outputId": "26782e97-07b2-4311-bc8e-9b05052d43e2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   label                                            message\n",
            "0  ham    Vá até o ponto de Jurong, louco. Disponível ap...\n",
            "1  ham                       Ok, lar... Brincando com vc...\n",
            "2  spam   Entrada gratuita em 2 competições semanais par...\n",
            "3  ham    Você não disse tão cedo... Você já disse então...\n",
            "4  ham    Não, acho que ele não estuda na USF, mas ele m...\n",
            "   label                                            message\n",
            "0  ham    Go until jurong point, crazy.. Available only ...\n",
            "1  ham                        Ok lar... Joking wif u oni...\n",
            "2  spam   Free entry in 2 a wkly comp to win FA Cup fina...\n",
            "3  ham    U dun say so early hor... U c already then say...\n",
            "4  ham    Nah I don't think he goes to usf, he lives aro...\n",
            "   label                                            message\n",
            "0  ham    Ve hasta jurong point, loco.. Disponible solo ...\n",
            "1  ham             Ok lar... Estoy bromeando contigo oni...\n",
            "2  spam   Entrada gratuita en 2 concursos semanales para...\n",
            "3  ham    No lo dijiste tan temprano... Ya lo dijiste en...\n",
            "4  ham    No, no creo que vaya a la USF, aunque vive por...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pré-processar cada dataset separadamente\n",
        "portuguese_df = preprocess_dataset(portuguese_df, 'portuguese')\n",
        "english_df = preprocess_dataset(english_df, 'english')\n",
        "spanish_df = preprocess_dataset(spanish_df, 'spanish')\n",
        "\n",
        "# Adicionar uma coluna indicando o idioma\n",
        "portuguese_df['language'] = 'portuguese'\n",
        "english_df['language'] = 'english'\n",
        "spanish_df['language'] = 'spanish'\n"
      ],
      "metadata": {
        "id": "XbvFoMpkxUQM"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenar os datasets pré-processados\n",
        "final_df = pd.concat([portuguese_df, english_df, spanish_df], ignore_index=True)\n",
        "\n",
        "# Exibir as primeiras linhas do dataset\n",
        "print(final_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNMJixeFxVx7",
        "outputId": "e0360bd5-7fdc-4c04-bff5-f3cabd9c561d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   label                                            message  \\\n",
            "0  ham    Vá até o ponto de Jurong, louco. Disponível ap...   \n",
            "1  ham                       Ok, lar... Brincando com vc...   \n",
            "2  spam   Entrada gratuita em 2 competições semanais par...   \n",
            "3  ham    Você não disse tão cedo... Você já disse então...   \n",
            "4  ham    Não, acho que ele não estuda na USF, mas ele m...   \n",
            "\n",
            "                                      processed_text    language  \n",
            "0  v at pont jurong louc disponvel apen bug n gre...  portuguese  \n",
            "1                                    ok lar brinc vc  portuguese  \n",
            "2  entrad gratuit competi seman venc final fa cup...  portuguese  \n",
            "3                    voc diss to ced voc j diss ento  portuguese  \n",
            "4                             acho estud usf mor aqu  portuguese  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Salvar o dataset final em um arquivo CSV\n",
        "final_df.to_csv('preprocessed_dataset.csv', index=False)\n",
        "print(\"Dataset final salvo como 'preprocessed_dataset.csv'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Lmfy9yCxXbA",
        "outputId": "3cdb4577-8d0d-4120-c933-d4f60cfb2375"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset final salvo como 'preprocessed_dataset.csv'.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}